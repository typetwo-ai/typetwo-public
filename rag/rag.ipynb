{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04310bcf-6204-4a8a-a2af-db2261592afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class RAGRetrieval:\n",
    "    def __init__(\n",
    "            self,\n",
    "            txt_directory: str,\n",
    "            chunk_size: int = 1000,\n",
    "            chunk_overlap: int = 200,\n",
    "            embedding_model_name: str = \"ibm-granite/granite-embedding-107m-multilingual\"\n",
    "    ):\n",
    "        self.txt_directory = txt_directory\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self.vectorstore = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        print(f\"Loading TXT documents from {self.txt_directory}\")\n",
    "\n",
    "        loader = DirectoryLoader(\n",
    "            self.txt_directory,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "\n",
    "        self.documents = loader.load()\n",
    "        print(f\"Loaded {len(self.documents)} documents\")\n",
    "\n",
    "        return self.documents\n",
    "\n",
    "    def chunk_documents(\n",
    "            self,\n",
    "            chunk_size: Optional[int] = None,\n",
    "            chunk_overlap: Optional[int] = None\n",
    "    ) -> List[Document]:\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "\n",
    "        chunk_size = chunk_size or self.chunk_size\n",
    "        chunk_overlap = chunk_overlap or self.chunk_overlap\n",
    "\n",
    "        print(f\"Chunking documents with size={chunk_size}, overlap={chunk_overlap}\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "\n",
    "        self.chunks = text_splitter.split_documents(self.documents)\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} chunks\")\n",
    "\n",
    "        return self.chunks\n",
    "\n",
    "    def initialize_embeddings(self, model_name: Optional[str] = None) -> None:\n",
    "        model_name = model_name or self.embedding_model_name\n",
    "        print(f\"Initializing embeddings with model: {model_name}\")\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\" if os.environ.get(\"USE_CUDA\", \"0\") == \"1\" else \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "\n",
    "    def create_vectorstore(self) -> FAISS:\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No chunks available. Call chunk_documents() first.\")\n",
    "\n",
    "        if not self.embeddings:\n",
    "            self.initialize_embeddings()\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "\n",
    "        self.vectorstore = FAISS.from_documents(\n",
    "            self.chunks,\n",
    "            self.embeddings\n",
    "        )\n",
    "\n",
    "        print(\"Vector store created successfully\")\n",
    "        return self.vectorstore\n",
    "\n",
    "    def save_vectorstore(self, path: str) -> None:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"No vector store available. Call create_vectorstore() first.\")\n",
    "\n",
    "        print(f\"Saving vector store to {path}\")\n",
    "        self.vectorstore.save_local(path)\n",
    "\n",
    "    def load_vectorstore(self, path: str) -> FAISS:\n",
    "        if not self.embeddings:\n",
    "            self.initialize_embeddings()\n",
    "\n",
    "        print(f\"Loading vector store from {path}\")\n",
    "        self.vectorstore = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        return self.vectorstore\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10, score_threshold: Optional[float] = None) -> List[Document]:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"No vector store available. Call create_vectorstore() first.\")\n",
    "\n",
    "        print(f\"Retrieving top {k} chunks for query: '{query}'\")\n",
    "\n",
    "        docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        if score_threshold is not None:\n",
    "            docs_and_scores = [(doc, score) for doc, score in docs_and_scores if score >= score_threshold]\n",
    "\n",
    "        retrieved_docs = []\n",
    "        for doc, score in docs_and_scores:\n",
    "            doc.metadata[\"similarity_score\"] = float(score)\n",
    "            retrieved_docs.append(doc)\n",
    "\n",
    "        print(f\"Retrieved {len(retrieved_docs)} chunks\")\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7773d60-9d86-4cf0-851e-1f56a56bf11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TXT documents from /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed\n",
      "Loaded 2 documents\n",
      "Chunking documents with size=1000, overlap=100\n",
      "Created 184 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = RAGRetrieval(\n",
    "    txt_directory=\"/Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "retriever.load_documents()\n",
    "retriever.chunk_documents()\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7eec3c3-727d-4bf2-90f7-a5b26e5a289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_out = []\n",
    "\n",
    "for chunk in retriever.chunks:\n",
    "    json_out.append({\n",
    "        'file': chunk.metadata.get('source'),\n",
    "        'txt': chunk.page_content,\n",
    "        'start_index': chunk.metadata.get('start_index')\n",
    "    })\n",
    "\n",
    "with open('debug_chunks.json', 'w') as f:\n",
    "    f.write(json.dumps(json_out, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c894bfbc-d98e-4b7a-bbc2-2f2d9cb363d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings with model: ibm-granite/granite-embedding-107m-multilingual\n",
      "Creating vector store...\n",
      "Vector store created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x38791b790>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.create_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee215b08-dd9d-4d92-b96d-19283eff4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 10 chunks for query: '21181-14984'\n",
      "Retrieved 10 chunks\n",
      "####################################\n",
      "Chunk 1:\n",
      "Content: |---------------------------------------------------------------------------------------------------\n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.6356346607208252\n",
      "####################################\n",
      "Chunk 2:\n",
      "Content: ̈\n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.6444354057312012\n",
      "####################################\n",
      "Chunk 3:\n",
      "Content: ̈\n",
      "\n",
      "̈\n",
      "\n",
      "Saskia Klein -Heinrich Heine University Dusseldorf, Faculty of Mathematics and Natural Science\n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7086974382400513\n",
      "####################################\n",
      "Chunk 4:\n",
      "Content: Phone: (+49) 21181-14984; Email: Thomas.kurz@hhu.de\n",
      "\n",
      "## Authors\n",
      "\n",
      "̈\n",
      "\n",
      "Tanja C. Knaab -Heinrich Heine U\n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7238383293151855\n",
      "####################################\n",
      "Chunk 5:\n",
      "Content: | Dd2 SJ557733         | Pf atp4         |                      0.011 |              0.246 |        \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7250418663024902\n",
      "####################################\n",
      "Chunk 6:\n",
      "Content: | 2 (TKK130)   | 20 ± 2                | 10,000                 | 89.3 (89.4; 89.1)                 \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7537293434143066\n",
      "####################################\n",
      "Chunk 7:\n",
      "Content: = 14.4 Hz), 117.91 (d, 4 J CF = 5.1 Hz), 117.19 (d, 2 J CF = 23.1 Hz), 105.73 (dd, 2 J CF = 22.8, 4 \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7582285404205322\n",
      "####################################\n",
      "Chunk 8:\n",
      "Content: | 1 - 1.25 ×      | Not a DDI                       | No risk                                       \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/1021-1032.txt\n",
      "Similarity Score: 0.7600384950637817\n",
      "####################################\n",
      "Chunk 9:\n",
      "Content: | LUM                                                                                               \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7614314556121826\n",
      "####################################\n",
      "Chunk 10:\n",
      "Content: | Hf                                                                                                \n",
      "Source: /Users/alex/typetwo-public/rag/data/2025 Volume 68 (subset) Parsed/0095-0107.txt\n",
      "Similarity Score: 0.7690340280532837\n"
     ]
    }
   ],
   "source": [
    "query = '21181-14984'\n",
    "\n",
    "retrieved_chunks = retriever.retrieve(query, k=10)\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print('####################################')\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    print(f\"Content: {chunk.page_content[:100]}\")\n",
    "    # print(chunk)\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Similarity Score: {chunk.metadata.get('similarity_score', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9423a61-0f42-4bcd-a01d-2645b90a8a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mretrieved_chunks\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieved_chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a87ac-bbf0-4c82-bb7c-e972ff1e4576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
