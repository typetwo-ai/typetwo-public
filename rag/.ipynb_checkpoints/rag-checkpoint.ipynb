{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04310bcf-6204-4a8a-a2af-db2261592afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class RAGRetrieval:\n",
    "    \"\"\"\n",
    "    A configurable retrieval component for RAG pipelines with fine-grained control over each step.\n",
    "    Uses LangChain for document loading and chunking, with granite-embedding-107m-multilingual for embeddings.\n",
    "    Modified to work with TXT files instead of PDFs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            txt_directory: str,\n",
    "            chunk_size: int = 1000,\n",
    "            chunk_overlap: int = 200,\n",
    "            embedding_model_name: str = \"ibm-granite/granite-embedding-107m-multilingual\"\n",
    "    ):\n",
    "        self.txt_directory = txt_directory\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "\n",
    "        self.documents = []\n",
    "        self.chunks = []\n",
    "        self.vectorstore = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        print(f\"Loading TXT documents from {self.txt_directory}\")\n",
    "\n",
    "        loader = DirectoryLoader(\n",
    "            self.txt_directory,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "\n",
    "        self.documents = loader.load()\n",
    "        print(f\"Loaded {len(self.documents)} documents\")\n",
    "\n",
    "        return self.documents\n",
    "\n",
    "    def chunk_documents(\n",
    "            self,\n",
    "            chunk_size: Optional[int] = None,\n",
    "            chunk_overlap: Optional[int] = None\n",
    "    ) -> List[Document]:\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents loaded. Call load_documents() first.\")\n",
    "\n",
    "        chunk_size = chunk_size or self.chunk_size\n",
    "        chunk_overlap = chunk_overlap or self.chunk_overlap\n",
    "\n",
    "        print(f\"Chunking documents with size={chunk_size}, overlap={chunk_overlap}\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            add_start_index=True,  # Track the start index of each chunk\n",
    "        )\n",
    "\n",
    "        self.chunks = text_splitter.split_documents(self.documents)\n",
    "        print(f\"Created {len(self.chunks)} chunks\")\n",
    "\n",
    "        return self.chunks\n",
    "\n",
    "    def initialize_embeddings(self, model_name: Optional[str] = None) -> None:\n",
    "        model_name = model_name or self.embedding_model_name\n",
    "        print(f\"Initializing embeddings with model: {model_name}\")\n",
    "\n",
    "        # Create HuggingFace embeddings\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\" if os.environ.get(\"USE_CUDA\", \"0\") == \"1\" else \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "\n",
    "    def create_vectorstore(self) -> FAISS:\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No chunks available. Call chunk_documents() first.\")\n",
    "\n",
    "        if not self.embeddings:\n",
    "            self.initialize_embeddings()\n",
    "\n",
    "        print(\"Creating vector store...\")\n",
    "\n",
    "        self.vectorstore = FAISS.from_documents(\n",
    "            self.chunks,\n",
    "            self.embeddings\n",
    "        )\n",
    "\n",
    "        print(\"Vector store created successfully\")\n",
    "        return self.vectorstore\n",
    "\n",
    "    def save_vectorstore(self, path: str) -> None:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"No vector store available. Call create_vectorstore() first.\")\n",
    "\n",
    "        print(f\"Saving vector store to {path}\")\n",
    "        self.vectorstore.save_local(path)\n",
    "\n",
    "    def load_vectorstore(self, path: str) -> FAISS:\n",
    "        if not self.embeddings:\n",
    "            self.initialize_embeddings()\n",
    "\n",
    "        print(f\"Loading vector store from {path}\")\n",
    "        self.vectorstore = FAISS.load_local(path, self.embeddings)\n",
    "        return self.vectorstore\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10, score_threshold: Optional[float] = None) -> List[Document]:\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"No vector store available. Call create_vectorstore() first.\")\n",
    "\n",
    "        print(f\"Retrieving top {k} chunks for query: '{query}'\")\n",
    "\n",
    "        docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        if score_threshold is not None:\n",
    "            docs_and_scores = [(doc, score) for doc, score in docs_and_scores if score >= score_threshold]\n",
    "\n",
    "        retrieved_docs = []\n",
    "        for doc, score in docs_and_scores:\n",
    "            doc.metadata[\"similarity_score\"] = float(score)\n",
    "            retrieved_docs.append(doc)\n",
    "\n",
    "        print(f\"Retrieved {len(retrieved_docs)} chunks\")\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7773d60-9d86-4cf0-851e-1f56a56bf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee215b08-dd9d-4d92-b96d-19283eff4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = retriever.retrieve(query, k=10)\n",
    "\n",
    "# Print retrieved chunks with their metadata\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"\\nChunk {i + 1}:\")\n",
    "    print(f\"Content: {chunk.page_content[:150]}...\")\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Similarity Score: {chunk.metadata.get('similarity_score', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
